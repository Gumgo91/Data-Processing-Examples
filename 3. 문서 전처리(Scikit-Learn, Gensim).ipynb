{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn의 문서 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW (Bag of Words)\n",
    "- 문서를 숫자 벡터로 변환하는 가장 기본적인 방법\n",
    "- 전체 문서(코커스=말뭉치) {d1, d2, ..., dn}를 구성하는 고정된 단어장(vocabulary) {t1, t2, ..., tm} 를 만들고, di라는 개별 문서에 단어장의 단어들이 포함되어 있는지를 표시하는 방법.\n",
    "- x(i,j) = 문서 di 내의 단어 tj의 출현 빈도\n",
    "- 또는 x(i,j) = 문서 di 내에 단어 tj가 있으면 1, 없으면 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "- 문서를 벡터로 바꾸는 전처리용 클래스\n",
    "- DictVectorizer: 각 단어 수를 세어놓은 사전에서 BOW 벡터를 만듦\n",
    "- CountVectorizer: 문서 집합에서 단어 토큰을 생성하고, 각 단어의 수를 세어 BOW 인코딩한 벡터를 만듦\n",
    "- TfidfVectorizer: CountVectorizer와 유사. but, TF_IDF 방식으로 단어 가중치를 조정\n",
    "- HashingVectorizer: 해시 함수(hash function)을 사용하여 적은 메모리 사용. 빠른 속도로 BOW 벡터를 만듦.\n",
    "\n",
    "BOW인코딩 결과는 Sparse 행렬로 만들어지므로, toarray 메서드로 보통 행렬로 변환해야함.\n",
    "(저장 시 0을 무시하고 저장하는 방법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DictVectorizer\n",
    "- 가장 간단한 형태\n",
    "- 각 문서에 대해 워드를 잘라 토큰화를 하고, 몇번나왔는지를 세는것이 불편."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A':1, 'B':2}, {'B':3, 'C':1}] #어떤 단어가 몇번 나왔는지를 수동으로 세서 DIctionary로 만들어 놓아야 함.\n",
    "#첫번째 문서에는 A라는 단어가 1번, B라는 단어가 2번 쓰였고, 두번째 문서에는 B가 3번, C가 1번 쓰였다.\n",
    "X = v.fit_transform(D)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.feature_names_  #vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.transform({'C':4, 'D':3})\n",
    "#D가 들어가면, D는 없는 것으로 처리가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "- 문서를 토큰 리스트로 변환하고, 토큰의 출현 빈도를 세는 작업까지 알아서 해줌.\n",
    "- 문서를 넣으면 BOW 인코딩 벡터로 변환\n",
    "- 문서를 넣어주기만 하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Corpus에 리스트로 문서들을 넣어준다.\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?'\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_ #단어장. 단어와 index number를 dict로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.transform(['This is the second document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop_words: 특정 단어 무시\n",
    "\n",
    "#리스트로 넣어주기\n",
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "print(vect.vocabulary_)\n",
    "\n",
    "#미리 정해진 stopwords 세트 이용 가능. english = 영어에서 잘 이용되는 stopwords들\n",
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰생성기 선택: analyzer, tokenizer, token_pattern 등의 인수 이용\n",
    "\n",
    "#analyzer: 캐릭터단위로 선택 가능\n",
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_pattern: 정규표현식 이용 가능\n",
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer: 인수로 nltk에 있는 토크나이저 선택 가능\n",
    "import nltk\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-그램: 단어장 생성에 사용할 토큰 크기 결정.\n",
    "#모노그램(1-그램)은 토큰 하나만 단어로 사용, 바이그램(2-그램)은 두 개의 연결된 토큰을 하나의 단어로 사용\n",
    "#구를 토큰으로 사용하겠다 라는 의미.\n",
    "vect = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "vect.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도 수를 이용한 stop word 이용.\n",
    "#max_df, min_df 인수를 이용해, 문서에서 토큰이 나타난 횟수를 기준으로 단어장 구성 가능\n",
    "#max_df를 초과하거나, min_df보다 작은 경우에는 무시. (정수는 횟수, 부동소수점은 비중)\n",
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_ , vect.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFidVectorizer\n",
    "- Term Frequency - Inverse Document Frequency\n",
    "- 단어를 갯수 그대로 카운팅X, 공통적으로 들어 있는 단어의 경우 문서별 구분 능력 적다 판단하고 가중치를 축소하는 방법\n",
    "- 단어수에 idf(t)를 곱해줌.: inverse document frequency\n",
    "- idf(d, t) = log n / (1 + df(t) )\n",
    "- 중요도가 낮은 단어는 숫자가 더 내려감."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer\n",
    "- Hash Function를 이용해서 문자열을 숫자로 변환시킴\n",
    "- 처리할 문서가 커지면 단어장이 커지는데, hash trick 이용하면 문자열 검색 부분이 없기 때문에 시간이 유리\n",
    "- 단어에 대한 인덱스 번호를 수식으로 생성\n",
    "- 사전메모리 없고 실행 시간 줄일 수 있음.\n",
    "- But, 충돌이 생기는 경우가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim의 문서 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Collecting Cython==0.29.14\n",
      "  Using cached Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Processing c:\\users\\ragnarokv\\appdata\\local\\pip\\cache\\wheels\\56\\b5\\6d\\86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648\\smart_open-2.1.0-py3-none-any.whl\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.14.43-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: boto in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Collecting botocore<1.18.0,>=1.17.43\n",
      "  Using cached botocore-1.17.43-py2.py3-none-any.whl (6.5 MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.43->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Installing collected packages: Cython, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed Cython-0.29.14 boto3-1.14.43 botocore-1.17.43 docutils-0.15.2 gensim-3.8.3 s3transfer-0.3.3 smart-open-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\ragnarokv\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install --user gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim의 BOW 인코딩\n",
    "- 토픽 모델링을 할 때 이용\n",
    "- Dictionary class 이용\n",
    " - token2id 속성으로 사전 저장\n",
    " - doc2bow 메서드로 BOW 인코딩\n",
    "- TfidModel 클래스 이용하면 TF-IDF 인코딩도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'the', 'first', 'document.'],\n",
       " ['This', 'is', 'the', 'second', 'document.'],\n",
       " ['And', 'the', 'third', 'one.'],\n",
       " ['Is', 'this', 'the', 'first', 'document?'],\n",
       " ['The', 'last', 'document?']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#copus 만들기\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?'\n",
    "]\n",
    "\n",
    "#토큰 리스트 생성\n",
    "token_list = [[text for text in doc.split()] for doc in corpus]\n",
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 0,\n",
       " 'document.': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'the': 4,\n",
       " 'second': 5,\n",
       " 'And': 6,\n",
       " 'one.': 7,\n",
       " 'third': 8,\n",
       " 'Is': 9,\n",
       " 'document?': 10,\n",
       " 'this': 11,\n",
       " 'The': 12,\n",
       " 'last': 13}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DIctionary 객체 생성\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(token_list)\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(0, 1), (1, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(4, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(2, 1), (4, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(10, 1), (12, 1), (13, 1)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW 인코딩\n",
    "term_matrix = [dictionary.doc2bow(token) for token in token_list]\n",
    "term_matrix\n",
    "#각 단어가 몇번 쓰였는지 sparse matrix형태로 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc:\n",
      "0 0.49633406058198626\n",
      "1 0.49633406058198626\n",
      "2 0.49633406058198626\n",
      "3 0.49633406058198626\n",
      "4 0.12087183801361165\n",
      "doc:\n",
      "0 0.4034194772828018\n",
      "1 0.4034194772828018\n",
      "3 0.4034194772828018\n",
      "4 0.09824442362969368\n",
      "5 0.7085945309359098\n",
      "doc:\n",
      "4 0.07979258234193365\n",
      "6 0.5755093812740171\n",
      "7 0.5755093812740171\n",
      "8 0.5755093812740171\n",
      "doc:\n",
      "2 0.3485847413542797\n",
      "4 0.08489056411237639\n",
      "9 0.6122789185961829\n",
      "10 0.3485847413542797\n",
      "11 0.6122789185961829\n",
      "doc:\n",
      "10 0.37344696513776354\n",
      "12 0.6559486886294514\n",
      "13 0.6559486886294514\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF 인코딩\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(term_matrix) #카운팅 매트릭스를 그대로 넣게 되면, tfid로 바뀜.\n",
    "\n",
    "for doc in tfidf[term_matrix]:\n",
    "    print(\"doc:\")\n",
    "    for k, v in doc:\n",
    "        print(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
